{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Packages"
      ],
      "metadata": {
        "id": "MKbveQCaYMT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from torchsummary import summary\n",
        "import torchvision\n",
        "from torchvision.models import squeezenet1_1, SqueezeNet1_1_Weights\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMi0sfQKYJyC",
        "outputId": "a3dff8cd-214b-4313-b812-2359984e5aae"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeRbvxXs4XtM",
        "outputId": "fb6a5201-5067-439e-de78-e697fcd09081"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU: NVIDIA L4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a random seed for reproducibility\n",
        "def set_seed(seed_value=42):\n",
        "    random.seed(seed_value)       # Python random module\n",
        "    np.random.seed(seed_value)    # Numpy module\n",
        "    torch.manual_seed(seed_value) # Torch\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed_value)  # Environment variable\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)  # if using multi-GPU\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(24)"
      ],
      "metadata": {
        "id": "zkPKFa06sCiL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the Dataset"
      ],
      "metadata": {
        "id": "tG1vjZlxa008"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "KJL0iSOeY591"
      },
      "outputs": [],
      "source": [
        "'TODO: Define transformations - crop or resize'\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization for pre-trained models\n",
        "])\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "In order to load the datasets from the shared folder, go to google drive, right click the shared folder, and create a shortcut\n",
        "to somewhere in your drive.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Ben's dataset paths\n",
        "ben_train_dataset_path = \"/content/gdrive/MyDrive/24S Classes/Deep Learning/COSC78 Final Project/Train Data\"\n",
        "ben_validation_dataset_path = \"/content/gdrive/MyDrive/24S Classes/Deep Learning/COSC78 Final Project/Validation Data\"\n",
        "ben_test_dataset_path = \"/content/gdrive/MyDrive/24S Classes/Deep Learning/COSC78 Final Project/Test Data\"\n",
        "\n",
        "\n",
        "# Dawson's dataset paths\n",
        "daw_train_dataset_path = '/content/gdrive/MyDrive/COSC78 Final Project/Train Data'\n",
        "daw_validation_dataset_path = '/content/gdrive/MyDrive/COSC78 Final Project/Validation Data'\n",
        "daw_test_dataset_path = '/content/gdrive/MyDrive/COSC78 Final Project/Test Data'\n",
        "\n",
        "# Will's dataset paths\n",
        "will_train_dataset_path = '/content/gdrive/MyDrive/COSC78/COSC78 Final Project/Train Data'\n",
        "will_validation_dataset_path = '/content/gdrive/MyDrive/COSC78/COSC78 Final Project/Validation Data'\n",
        "will_test_dataset_path = '/content/gdrive/MyDrive/COSC78/COSC78 Final Project/Test Data'\n",
        "\n",
        "# Brian's dataset paths\n",
        "bri_train_dataset_path = '/content/gdrive/MyDrive/Algorithms - Collab/CS 78/COSC78 Final Project/Train Data'\n",
        "bri_val_dataset_path = '/content/gdrive/MyDrive/Algorithms - Collab/CS 78/COSC78 Final Project/Validation Data'\n",
        "bri_test_dataset_path = '/content/gdrive/MyDrive/Algorithms - Collab/CS 78/COSC78 Final Project/Test Data'\n",
        "\n",
        "#% Dataset paths in use %# (currently brian's)\n",
        "train_dataset_path = bri_train_dataset_path\n",
        "validation_dataset_path = bri_val_dataset_path\n",
        "test_dataset_path = bri_test_dataset_path\n",
        "\n",
        "\n",
        "generator = torch.Generator(device=device)\n",
        "\n",
        "\n",
        "with torch.device(device):\n",
        "    # Setup datasets using ImageFolder\n",
        "    train_dataset = datasets.ImageFolder(train_dataset_path, transform=transform)\n",
        "    val_dataset = datasets.ImageFolder(validation_dataset_path, transform=transform)\n",
        "    test_dataset = datasets.ImageFolder(test_dataset_path, transform=transform)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True, generator=generator)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=2048, shuffle=False, generator=generator)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False, generator=generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import and modify the model"
      ],
      "metadata": {
        "id": "yV1U_YB_bdvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained SqueezeNet 1_1 model\n",
        "pretrained_weights = squeezenet1_1(pretrained=True)\n",
        "squeezenet = pretrained_weights\n",
        "\n",
        "# Freeze all the parameters for fine-tuning\n",
        "for param in squeezenet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the classifier head\n",
        "num_classes = 3  # Number of classes in your dataset\n",
        "squeezenet.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "squeezenet.classifier[1].requires_grad_ = True\n",
        "\n",
        "# Print the model summary\n",
        "# summary(squeezenet, input_size=(3, 224, 224))"
      ],
      "metadata": {
        "id": "Bc6PMZa9bhVI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32a504a0-f8e5-4eee-ffc1-a74f5293949c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_1_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_1_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk1BMAf1sak6"
      },
      "source": [
        "## Fine tune the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(squeezenet.parameters())"
      ],
      "metadata": {
        "id": "Vgoyy06m2Pj1"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "H60gXIg5Rheb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba1f9ee6-425e-44b3-fa8c-5ebe4897cfb1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7efa1c48ea50>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(squeezenet.parameters())\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # Decreases the learning rate by a factor of 0.1 every 10 epochs\n",
        "torch.manual_seed(24)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_validate(model, train_loader, val_loader, optimizer, loss_func, epochs=25):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs} - Training\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = loss_func(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * images.size(0)\n",
        "\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        history['train_loss'].append(train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{epochs} - Validation\"):\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = loss_func(outputs, labels)\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        history['val_loss'].append(val_loss)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "sDxNp48Y31Tm"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.device(device):\n",
        "  history = train_and_validate(squeezenet, train_loader, val_loader, optimizer, loss_func, epochs=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9T2yJ7d2UMu",
        "outputId": "b12e99fb-f74c-45d8-e128-9355b2684759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/25 - Training:  58%|█████▊    | 7/12 [27:15<24:26, 293.28s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "SmcGpmZvR1An",
        "outputId": "c7205bb8-acc0-4cbe-bf26-dfd5db1abefd"
      },
      "outputs": [],
      "source": [
        "history = train_and_validate(squeezenet, train_loader, val_loader, optimizer, scheduler, loss_func, epochs=25)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting training and validation accuracy\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(history['train_accuracy'], label='Train Accuracy')\n",
        "plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "\n",
        "# Plotting training and validation F1 score\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(history['train_f1'], label='Train F1 Score')\n",
        "plt.plot(history['val_f1'], label='Validation F1 Score')\n",
        "plt.title('Model F1 Score')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.legend()\n",
        "\n",
        "# Plotting training and validation loss\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(history['train_loss'], label='Train Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U92IqWAQDSpK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
